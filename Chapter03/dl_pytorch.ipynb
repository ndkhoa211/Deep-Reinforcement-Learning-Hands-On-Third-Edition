{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0b5c3fd",
   "metadata": {},
   "source": [
    "# Deep Learning with Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265ab6cf",
   "metadata": {},
   "source": [
    "We'll cover:\n",
    "1. the Pytorch library specifics and implementation details\n",
    "2. Higher-level libraries on top of Pytorch, with the aim of simplifying common DL problems\n",
    "3. the Pytorch Ignite library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a365e637",
   "metadata": {},
   "source": [
    "`Tensor` is just a multi-dimensional array\n",
    "1. tensors in DL are only partially related to tensors used in `tensor calculus` or `tensor algebra`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c12998",
   "metadata": {},
   "source": [
    "## 2. The creation of tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104490ab",
   "metadata": {},
   "source": [
    "1. Apart from dimensions, a tensor is characterized by the type of its elements. There are $13$ types supported by Pytorch:\n",
    "    1. $4$ float types:\n",
    "        - 16-bit, with $2$ variants:\n",
    "            - `float16` has more bits for precision\n",
    "            - `bfloat16` has larger exponent part\n",
    "        - 32-bit\n",
    "        - 64-bit\n",
    "    2. $3$ complex types:\n",
    "        - 32-bit\n",
    "        - 64-bit\n",
    "        - 128-bit\n",
    "    3. $5$ integer types:\n",
    "        - 8-bit signed\n",
    "        - 8-bit unsigned\n",
    "        - 16-bit signed\n",
    "        - 32-bit signed\n",
    "        - 64-bit signed\n",
    "    4. Boolean type\n",
    "\n",
    "2. There are also $4$ `quantized number` types, but they are using the preceding types, just with different bit representation and interpretation\n",
    "\n",
    "3. Tensors of different types are represented by different classes, with the most commonly used being\n",
    "    1. `torch.FloatTensor` (corresponding to a 32-bit float)\n",
    "    2. `torch.ByteTensor` (an 8-bit unsigned integer)\n",
    "    3. `torch.LongTensor` (a 64-bit signed integer)\n",
    "    4. Refer to documentation for other tensor types' names\n",
    "\n",
    "4. $3$ ways to create a tensor in Pytorch:\n",
    "    1. calling a constructor of the required type\n",
    "        - or we can also provide a Python iterable (e.g. list or tuple) to the constructor\n",
    "    2. asking Pytorch to create a tensor with specific data\n",
    "    3. converting Numpy array or a Python list to a tensor using `torch.tensor()`\n",
    "        - by default, the converted tensor has type of 64-bit float (or double) `torch.float64`, i.e `DoubleTensor` type\n",
    "        - usually in DL, double precision is not required and it adds an extra memory and performance overhead\n",
    "        - common practice: use 32-bit float type, or even 16-bit float type\n",
    "\n",
    "5. There are $2$ types of operation for tensors:\n",
    "    1. `inplace` operations have an underscore appended to their name and operate on the tensor's content. After this, the object itself is returned\n",
    "        - more efficient from a performance/memory point of view, but modification of an existing tensor might lead to hidden bugs\n",
    "    2. `functional` operations create a copy of the tensor with the performed modification, leaving the original tensor untouched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b98416b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1\n",
    "import torch\n",
    "import numpy as np\n",
    "a = torch.FloatTensor(3, 2)\n",
    "a # Pytorch initialize with zero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a92075b",
   "metadata": {},
   "source": [
    "As we can see, Pytorch initializes memory with zeros, which is a different behavior from previous versions:\n",
    "- before, it just allocated memory and kept it uninitialized, which is slightly faster but less safe (as it might introduce tricky bugs and security issues)\n",
    "- but we shouldn't rely on this behavior, as it might change again (or behave differently on different hardware backend) and always initialize the contents of the tensor\n",
    "- to do so, we can either use one the the tensor construct operators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c859db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# functional operation\n",
    "torch.zeros(3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "940bc08c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inplace operation\n",
    "a.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bd6a07f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [3., 2., 1.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# provide a Python iterable\n",
    "torch.FloatTensor([[1,2,3],[3,2,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d1d6b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy array:\n",
      " [[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert from Numpy array\n",
    "n = np.zeros(shape=(3,2))\n",
    "print(\"Numpy array:\\n\", n)\n",
    "\n",
    "b = torch.tensor(n)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ad8c02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert Numpy array to 32-bit float type tensor\n",
    "n = np.zeros(shape=(3,2), dtype=np.float32)\n",
    "torch.tensor(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d208bb99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert Numpy array to 32-bit float type tensor\n",
    "n = np.zeros(shape=(3,2))\n",
    "torch.tensor(n, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7e7fde",
   "metadata": {},
   "source": [
    "### 2.1 Scalar tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18415c72",
   "metadata": {},
   "source": [
    "Zero-dimensional tensors can be created by the `torch.tensor()` function. To access the actual Python value of scalar tensor, we can use the special `item()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e43df94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c45171a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum:  tensor(6)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([1,2,3]).sum()\n",
    "print(\"Sum: \", a)\n",
    "a.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa735bf",
   "metadata": {},
   "source": [
    "### 2.2 Tensor operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c229854",
   "metadata": {},
   "source": [
    "1. Refer to Pytorch documentation. There are $2$ places to look for operations:\n",
    "    1. the `torch` package: the function usually accepts the tensor as an argument\n",
    "    2. the `tensor` class: the function operates on the called tensor\n",
    "\n",
    "2. Most of the time, tensor operations in Pytorch are trying to correspond to their Numpy equivalent\n",
    "    - examples: `torch.stack()`, `torch.transpose()`, `torch.cat()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca84154",
   "metadata": {},
   "source": [
    "### 2.3 GPU tensors (run in Google Colab to access to GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29295552",
   "metadata": {},
   "source": [
    "1. Pytorch supports CUDA GPUs, i.e. all operations have $2$ versions - CPU and GPU\n",
    "    - every tensor type we mentioned so far is for CPU and has its GPU equivalent\n",
    "        - GPU tensors reside in the `torch.cuda` package (e.g. `torch.cuda.FloatTensor`)\n",
    "        - CPU tensors reside in the `torch` package (e.g. `torch.FloatTensor`)\n",
    "\n",
    "2. In fact, under the hood, Pytorch supports not just CPU and CUDA; it has a notion of `backend`, which is an abstract computation device with memory\n",
    "    - tensors could be allocated in the backend's memory and computations could be performed on them\n",
    "    - example, on Apple hardware, Pytorch support `Metal Performance Shaders (MPS)` as backend called `mps`\n",
    "\n",
    "3. the tensor method `to(device)` creates a copy of the tensor to a specified device\n",
    "    - the device type can be specified in different ways:\n",
    "        1. pass string name of the device:\n",
    "            - `\"cpu\"` for CPU memory\n",
    "            - `\"cuda\"` for GPU\n",
    "            - `\"cuda:n\"` (`n` specifies the GPU index - zero-based)\n",
    "        2. use `torch.device` class in the `to()` method, which accepts the device name and optional index\n",
    "    - the old methods in previous version still works: `cpu()`, `cuda()`\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "78ca2ee0",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m a = torch.FloatTensor([\u001b[32m2\u001b[39m,\u001b[32m3\u001b[39m])\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m ca = \u001b[43ma\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcuda\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCPU tensor:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m, a)\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mGPU tensor:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m, ca)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\Desktop\\Deep-Reinforcement-Learning-Hands-On-Third-Edition\\.venv\\Lib\\site-packages\\torch\\cuda\\__init__.py:403\u001b[39m, in \u001b[36m_lazy_init\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    398\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    399\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    400\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmultiprocessing, you must use the \u001b[39m\u001b[33m'\u001b[39m\u001b[33mspawn\u001b[39m\u001b[33m'\u001b[39m\u001b[33m start method\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    401\u001b[39m     )\n\u001b[32m    402\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch._C, \u001b[33m\"\u001b[39m\u001b[33m_cuda_getDeviceCount\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m403\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mTorch not compiled with CUDA enabled\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    404\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    405\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[32m    406\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    407\u001b[39m     )\n",
      "\u001b[31mAssertionError\u001b[39m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "a = torch.FloatTensor([2,3])\n",
    "\n",
    "ca = a.to(\"cuda\")\n",
    "\n",
    "print(\"CPU tensor:\\n\", a)\n",
    "print(\"GPU tensor:\\n\", ca)\n",
    "print(\"CPU tensor computation:\\n\", a+1)\n",
    "print(\"GPU tensor computation:\\n\", ca+1)\n",
    "print(\"Get tensor's device:\", ca.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf49944",
   "metadata": {},
   "source": [
    "## 3. Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775f108e",
   "metadata": {},
   "source": [
    "1. The `automatic computation gradients` feature/functionality was originally implemented in the Caffe toolkit and then became the de facto standard in DL libraries\n",
    "\n",
    "2. The direction of data and gradients flow during the optimization process: ![Data and gradients flowing through the NN](../images/figure_3-2.png)\n",
    "    - $2$ approaches on how gradients are calculated:\n",
    "        1. `Static graph`: need to define calculations in advance and it won't be possible to change them later\n",
    "            - the graph will be processed and optimized by the DL library before any computation is made\n",
    "            - was implemented in `TensorFlow < 2.0`, `Theano`, and many other DL toolkits\n",
    "            - strength:\n",
    "                1. faster, as all computations can be moved to GPY, minimizing data transfer overhead\n",
    "                2. the library has more freedom in optimizing the order that the computations are performed in or even removing parts of the graph\n",
    "        2. `Dynamic graph`: don't need to define graph in advance exactly as it will be executed; just need to execute operations that we want to use for data transformation on actual data\n",
    "            - during this, the library will record the order of the operations performed, and when being asked to calculate gradients, it will unroll history of operations, accumulating the gradients of network parameters\n",
    "            - this method is also called `notebook gradients`\n",
    "            - is implemented in `Pytorch`, `Chainer`, etc.\n",
    "            - strength:\n",
    "                1. although has higher computation overhead, it gives developers more freedom\n",
    "                2. (very appealing) allows us to express transformation more naturally and in a more Pythonic way \n",
    "\n",
    "3. `Pytorch 2.x` introduced `torch.compile` function, which speeds up Pytorch code by `JIT-compiling` the code into optimized kernels\n",
    "    - this is an evolution of the `TorchScript` and `FX Tracing` compiling methods from earlier versions\n",
    "    - from a historical perspective, this is highly amusing how originally radically different approaches of `TensorFlow (static graph)` and `Pytorch (dynamic graph)` are fusing into each other over time\n",
    "    - nowadays, Pytorch supports `compile()` and Tensorflow has `eager execution mode`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43da73d",
   "metadata": {},
   "source": [
    "### 3.1 Tensors and gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a62b3c",
   "metadata": {},
   "source": [
    "1. Pytorch tensors have a built-in gradient calculation and tracking machinery, so all we need to do is convert data into tensors and perform computations using the tensor methods and functions provided by `torch`\n",
    "\n",
    "2. several attributes related to gradients that every tensor has:\n",
    "    - `grad`: hold a tensor of the same shape containing computed gradients\n",
    "    - `is_leaf`: \n",
    "        1. `True` if constructed by the user\n",
    "        2. `False` if it was the result of function transformation (i.e. it has a parent in the computation graph)\n",
    "    - `requires_grad`:\n",
    "        1. `True` if requires gradients to be calculated\n",
    "            - this property is inherited from leaf tensors, which get this value from the tensor construction step (`torch.zeros()` or `torch.tensor()`, etc.)\n",
    "            - if one of the variables involved in computations is `True`, all subsequent nodes also have it\n",
    "        2. `False` default by the constructor\n",
    "\n",
    "3. for memory efficient, gradients are stored only for lead nodes with `requires_grade=True`\n",
    "    - call `retain_grad()` method to keep the gradients for non-lead nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9ddae796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum: tensor([3., 3.], grad_fn=<AddBackward0>)\n",
      "Result: 12.0\n",
      "v1 is leaf: True \n",
      "v2 is leaf: True\n",
      "v_sum is leaf: False \n",
      "v_res is leaf: False\n",
      "v1 requires grad: True \n",
      "v2 requires grad: False\n",
      "v_sum requires grad: True \n",
      "v_res requires grad: True\n"
     ]
    }
   ],
   "source": [
    "v1 = torch.tensor([1.0, 1.0], requires_grad=True)\n",
    "v2 = torch.tensor([2.0, 2.0])\n",
    "\n",
    "v_sum = v1 +v2\n",
    "print(f\"Sum: {v_sum}\")\n",
    "v_res = (v_sum*2).sum()\n",
    "print(f\"Result: {v_res}\")\n",
    "print(f\"v1 is leaf: {v1.is_leaf} \\nv2 is leaf: {v2.is_leaf}\")\n",
    "print(f\"v_sum is leaf: {v_sum.is_leaf} \\nv_res is leaf: {v_res.is_leaf}\")\n",
    "print(f\"v1 requires grad: {v1.requires_grad} \\nv2 requires grad: {v2.requires_grad}\")\n",
    "print(f\"v_sum requires grad: {v_sum.requires_grad} \\nv_res requires grad: {v_res.requires_grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200d754a",
   "metadata": {},
   "source": [
    "![Graph representation of the expression](../images/figure_3-3.png)\n",
    "\n",
    "Let's compute the gradients of our graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f73a34a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients of v1: tensor([2., 2.])\n",
      "Gradients of v2: None\n"
     ]
    }
   ],
   "source": [
    "v_res.backward()\n",
    "\n",
    "print(f\"Gradients of v1: {v1.grad}\")\n",
    "print(f\"Gradients of v2: {v2.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509eb520",
   "metadata": {},
   "source": [
    "## 4. NN building blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb453c3",
   "metadata": {},
   "source": [
    "1. `torch.nn` package has tons of predefined classes providing the basic functionality blocks\n",
    "    - all of them are designed with practice in mind (e.g. support mini-batches, have same default values, weights are properly initialized)\n",
    "    - all modules follow the convention of `callable`: instance of any class can act as a function when applied to its arguments\n",
    "\n",
    "2. all classes in the `torch.nn` package inherit from the `nn.Module` base class, which can be used to implement higher-level NN blocks. Some useful methods that all `nn.Module` children provide:\n",
    "    - `parameters()`: return an iterator of all variables that require gradient computation (i.e. module weight)\n",
    "    - `zero_grad()`: initialize all gradients of all parameters to zero\n",
    "    - `to(device)`: moves all module parameters to a given device\n",
    "    - `state_dict()`: return the dictionary with all module parameters and is useful for model serialization\n",
    "    - `load_state_dict()`: initialize the module with the state dictionary\n",
    "\n",
    "3. `Sequential` class combines other layers into the pipe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0d9ccd7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=2, out_features=5, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=5, out_features=20, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=20, out_features=10, bias=True)\n",
       "  (5): Dropout(p=0.3, inplace=False)\n",
       "  (6): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "s = nn.Sequential(\n",
    "    nn.Linear(2, 5),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(5, 20),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 10),\n",
    "    nn.Dropout(p=0.3),\n",
    "    nn.Softmax(dim=1)\n",
    ")\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c71d5d8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0856, 0.0722, 0.0618, 0.1016, 0.1192, 0.1016, 0.1534, 0.0887, 0.1278,\n",
       "         0.0880]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s(torch.FloatTensor([[1, 2]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdfcd72",
   "metadata": {},
   "source": [
    "## 5. Custom layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9c4d18",
   "metadata": {},
   "source": [
    "1. By subclassing `nn.Module` class, we can create our own building blocks, which can be stacked together, reused later, and integrated into the Pytorch framework flawlessly\n",
    "\n",
    "2. At its core, `nn.Module` provides rich functionality to its children:\n",
    "    1. track all submodules that the current module includes\n",
    "        - to keep track of(register) the submodule, we just need to assign it to the class's field\n",
    "    2. provide functions to deal with all parameters of the registered submodules:\n",
    "        - `parameters()` method: get full list of the module's parameters\n",
    "        - `zero_grads()` method: zero its gradients\n",
    "        - `to(device)` method\n",
    "        - `state_dict()` method: serialize the module\n",
    "        - `load_state_dict()` method: deserialize the module\n",
    "        - `apply()` method: perform generic transformation using our own callable\n",
    "    3. establish the convention of `Module` application to data\n",
    "        - every module needs to perform its data transformation in the `forward()` method by overriding it\n",
    "    4. advance use cases:\n",
    "        - functions with the ability to regiester a hook function to tweak module transformation or gradients flow\n",
    "\n",
    "3. to create a custom module, we usually have to do only $2$ things:\n",
    "    1. register submodules\n",
    "    2. implement the `forward()` method\n",
    "\n",
    "Refer to [modules script](01_modules.py):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "33c70e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class OurModule(nn.Module):\n",
    "    def __init__(self, num_inputs, num_classes, dropout_prob=0.3):\n",
    "        super(OurModule, self).__init__()\n",
    "        self.pipe = nn.Sequential(\n",
    "            nn.Linear(num_inputs, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(5, 20),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(20, num_classes),\n",
    "            nn.Dropout(p=dropout_prob),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pipe(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6f988b",
   "metadata": {},
   "source": [
    "1. **Line 6**: pass the parameters\n",
    "\n",
    "2. **Line 7**: call the parent's constructor to let it initialize itself\n",
    "\n",
    "3. **Line 8-16**: create `nn.Sequential` object/instance and assign it to our class/object's field named `pipe`\n",
    "    - by doing this, we'll automatically register this module (`nn.Sequential` inherits from `nn.Module`, as does everything in the `nn` package)\n",
    "    - after the constructor finishes, all those fields will be registered automatically\n",
    "    - `add_module()` function in `nn.Module` register submodules (useful if our module can have variable number of layers and they need to be created programmatically)\n",
    "\n",
    "4. **Line 18-19**: override `forward()` function with our implementation of data transformation\n",
    "    - **Line 19** askes `self.pipe` to transform the data\n",
    "    - note: to apply a module to the data, call the module as a callable (i.e. pretend that the module instance is a function and call it with the argument) and not use the `forward()` function of the `nn.Module` class\n",
    "        - this is because `nn.Module` overrides the `__call__()` method, which is being used when we treat an instance as callable\n",
    "        - this method does some `nn.Module` magic and calls our `forward()` method\n",
    "        - if we call `forward()` directly, we'll intervene with the `nn.Module` duty, which can give wron results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "62c619bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OurModule(\n",
      "  (pipe): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=5, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=5, out_features=20, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=20, out_features=3, bias=True)\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Softmax(dim=1)\n",
      "  )\n",
      ")\n",
      "tensor([[0.3673, 0.3164, 0.3164]], grad_fn=<SoftmaxBackward0>)\n",
      "Cuda's availability is False\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    net = OurModule(num_inputs=2, num_classes=3)\n",
    "    print(net)\n",
    "    v = torch.FloatTensor([[2, 3]])\n",
    "    out = net(v)\n",
    "    print(out)\n",
    "    print(\"Cuda's availability is %s\" % torch.cuda.is_available())\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"Data from cuda: %s\" % out.to('cuda'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1333a2a2",
   "metadata": {},
   "source": [
    "1. **Line 2-5**: create our module with the desired params, and ask it to transform the created data as callable\n",
    "\n",
    "2. **Line 6**: print network's structure (`nn.Module` overrides `__str__()` and `__repr__()`) to represent the inner structure in a nice way"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cecfbd",
   "metadata": {},
   "source": [
    "## 6. Loss functions and optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2036d980",
   "metadata": {},
   "source": [
    "1. We also need to define our learning objective: the `loss function`\n",
    "    - it accepts $2$ arguments: the network's output and the desired output\n",
    "    - it returns a single number: the `loss value` that measures how close the network's prediction is from the desired result\n",
    "    - we use the `loss value` to calculate gradients of network parameters and adjust them to decrease the loss value, which pushes our model to better results in the future"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f00f27",
   "metadata": {},
   "source": [
    "### 6.1 Loss functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4844ee1d",
   "metadata": {},
   "source": [
    "1. `loss functions` reside in the `nn` package and are implemented as an `nn.Module` subclass. The most commonly used standard loss functions:\n",
    "    1. `nn.MSELoss`: standard loss for regression problems\n",
    "    2. `nn.BCELoss` and `nn.BCEWithLogits`: for binary classification problems\n",
    "        - `nn.BCELoss` expects a single probability value (output of the Sigmoid layer)\n",
    "        - `nn.BCEWithLogits` assumes raw scores as input and applies Sigmoid itself - more numerically stable and efficient\n",
    "    3. `nn.CrossEntropyLoss` and `nn.NLLLoss`: maximum likelihood for multi-class classification problems\n",
    "        - `nn.CrossEntropyLoss` expects raw scores for each class and applies `LogSoftmax` internally\n",
    "        - `nn.NLLLoss` expects log probabilities as input\n",
    "    4. we are free to write our own `Module` subclass to compare output and target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912aab50",
   "metadata": {},
   "source": [
    "### 6.2 Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1136e1b6",
   "metadata": {},
   "source": [
    "1. Responsibility: take the gradients of the model params and change these params in order to decrease the loss value. Pytorch provides lots of popular optimizer implementations in the `torch.optim` package. Most widely known:\n",
    "    1. `SGD`: vanilla stochastic gradient descent algorithm with an optional momentum extension\n",
    "    2. `RMSprop`: proposed by Geoffrey Hinton\n",
    "    3. `Adagrad`: adaptive gradients optimizer\n",
    "    4. `Adam`: a combination of both `RMSprop` and `Adagrad`\n",
    "\n",
    "2. All optimizers expose the unified interface --> easy to experiment with different optimization methods\n",
    "    - on construction, we need to pass an iterable of tensors, which will be modifed during the optimization process\n",
    "    - usual practice: pass the result of `params()` call of the upper-level `nn.Module` instance, which will return an iterable of all lead tensors with gradients\n",
    "\n",
    "3. Let's discuss the common blueprint of a trainig loop:\n",
    "```python\n",
    "for batch_x, batch_y in iterate_batches(data, batch_size=N):\n",
    "    batch_x_t = torch.tensor(batch_x)\n",
    "    batch_y_t = torch.tensor(batch_y)\n",
    "    out_t = net(batch_x_t)\n",
    "    loss_t = loss_function(out_t, batch_y_t)\n",
    "    loss_t.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "```\n",
    "- **Line 1**: iterate over our data over and over again\n",
    "    - `epoch`: one interation over a full set of examples\n",
    "    - data is splitted into batches of equal size , since it is usually too large to fit into CPU or GPU memory at once\n",
    "- **Line 2-3**: every batch includes data samples and target labels, and both of them have to be tensor\n",
    "- **Line 4**: pass data samples to our network\n",
    "    - all transformations of our network are nothing more than a graph of our operations with intermediate tensor instances\n",
    "- **Line 5**: feed the network's output and target labels to the loss function\n",
    "    - all transformations inside the loss fuction are also a graph (can think of these $2$ graphs as one single combined computation graph)\n",
    "    - result: a tensor of one single loss value \n",
    "- **Line 6**: every tensor in this computation graph remembers its parent, so to calculate gradients for the whole network, call the `backward()` function on a loss function result\n",
    "    - the result of this call will be the unrolling of the graph of the performed computations and the calculating of gradients for every leaf tensor with `require_grad=True`\n",
    "    - usually, such tensors are our model's params: weights and biases of FFNs, and covolution filters\n",
    "    - every time a gradient is calculated, it is accumulated in the `tensor.grad` field, so one tensor can participate in a transformation multiple times and its gradients will be properly summed together\n",
    "    - example: one single RNN cell could be applied to multiple input items\n",
    "- **Line 7**: after `loss.backward()` call is finished, we have the gradients accumulated, and now it's time for the optimizer to do its job\n",
    "    - it takes all gradients from the params we have passed to it on construction and applied them\n",
    "    - all this is done with the method `step()`\n",
    "- **Line 8**: call `zero_grad()` to zero gradients of parameters\n",
    "    - can be placed at the beginning of the training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489f37e2",
   "metadata": {},
   "source": [
    "## 7. Monitoring with `TensorBoard`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30dbeff",
   "metadata": {},
   "source": [
    "1. A list of things that we should observe during training:\n",
    "    1. `loss value`: normally consists of several components like base loss and regularization losses\n",
    "        - should monitor both the total loss and the individual components over time\n",
    "    2. results of `validation` on training and test datasets.\n",
    "    3. statistics about gradients and weights.\n",
    "    4. values produced by the network:\n",
    "        - in a classification problem: measure the entropy of predicted class probabilities\n",
    "        - in a regression problem: raw predicted values can give tons of data about the training\n",
    "    5. `learning rates` and other hyperparameters, if they are adjusted over time\n",
    "\n",
    "2. the list could also include:\n",
    "    1. domain-specific metrics:\n",
    "        - word embedding projections\n",
    "        - audio samples\n",
    "        - images generated by GANs\n",
    "    2. monitor values related to training speed:\n",
    "        - how long and epoch takes (to see the effect of our optimizations or problems with hardware)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da39589",
   "metadata": {},
   "source": [
    "### 7.1 `TensorBoard` 101"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17edc79a",
   "metadata": {},
   "source": [
    "1. `TensorBoard`: originally developed as part of `TensorFlow` (later moved to a separate project, but still maintained by Google) to observe and analyze various NN characteristics during and after training\n",
    "    - a Python web service that we can start on our computer, passing it the directory where our training process will save values to be analyzed. Then we point our browser to `TensorBoard`'s board `6006`, and it shows us an interactive web interface with values updated in real time\n",
    "    - `TensorBoard` still uses the `TensorFlow` data format, so we'll need to write this data from our Pytorch program\n",
    "    - Pytorch support this data format with the `torch.utils.tensorboard` package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba81f93",
   "metadata": {},
   "source": [
    "### 7.2 Plotting metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565ac5eb",
   "metadata": {},
   "source": [
    "Refer to [tensorboard script](02_tensorboard.py):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "60ab42d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    funcs = {\"sin\": math.sin, \"cos\": math.cos, \"tan\": math.tan}\n",
    "\n",
    "    for angle in range(-360, 360):\n",
    "        angle_rad = angle * math.pi / 180\n",
    "        for name, fun in funcs.items():\n",
    "            val = fun(angle_rad)\n",
    "            writer.add_scalar(name, val, angle)\n",
    "\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e56f63",
   "metadata": {},
   "source": [
    "1. **Line 6**: by default, `SummaryWriter()` will create a unique directory in the `/runs` directory for every launch, to be able to compare different rounds of training\n",
    "    - the name of the new directory includes:\n",
    "        1. current date and time\n",
    "        2. host name\n",
    "    - to override this, pass the `log_dir` argument to `SummaryWriter()`\n",
    "    - can also add a suffix to the name of the directory by adding a `comment` argument\n",
    "        - e.g. to capture different experiments' semantics, such as `dropout=0.3` or `strong_regularisation`\n",
    "\n",
    "2. **Line 14**: every value is added to the writer using the `add_scalar()` function\n",
    "\n",
    "3. **Line 16**: close the writer\n",
    "    - note: the writer does a periodical flush (by default, every $2$ mins), so even in the case of a lengthy optimization process, we'll see our values\n",
    "    - use `flush()` method to flush `SummaryWriter()` data expiclitly\n",
    "\n",
    "4. The result of running this will be zero output on the console\n",
    "    - but we'll see a new directory created inside the `/runs` directory with a single file\n",
    "    - start `TensorBoard` and look at the result by:\n",
    "        1. open terminal\n",
    "        2. type `tensorboard --logdir runs` inside `/Chapter03` folder\n",
    "        3. open `http://localhost:6006/`\n",
    "    - to run on a remote server and make it accessible from other machines:\n",
    "        - `tensorboard --logdir runs -bind_all`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10dbb6fd",
   "metadata": {},
   "source": [
    "## 8. GAN on Atari images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249c4f43",
   "metadata": {},
   "source": [
    "1. To show the power of DL, as an example, let's train a GAN to generate screenshots of various Atari games\n",
    "    - practical usage of GAN: image quality improvement, realistic image generation, and feature learning\n",
    "    - no practical usage in this example, but it'll be a good showcase about everything we learned about Pytorch so far\n",
    "    - refer to [atari gan script](03_atari_gan.py):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f01d047",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import cv2\n",
    "import time\n",
    "import random\n",
    "import argparse\n",
    "import typing as tt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Register Atari environments\n",
    "import ale_py\n",
    "gym.register_envs(ale_py)\n",
    "\n",
    "# Configure gymnasium logger - set minimum level directly\n",
    "gym.logger.min_level = gym.logger.WARN  # Use gym.logger.ERROR for less verbose output\n",
    "\n",
    "LATENT_VECTOR_SIZE = 100\n",
    "DISCR_FILTERS = 64\n",
    "GENER_FILTERS = 64\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# dimension input image will be rescaled\n",
    "IMAGE_SIZE = 64\n",
    "\n",
    "LEARNING_RATE = 0.0001\n",
    "REPORT_EVERY_ITER = 100\n",
    "SAVE_IMAGE_EVERY_ITER = 1000\n",
    "\n",
    "\n",
    "class InputWrapper(gym.ObservationWrapper):\n",
    "    \"\"\"\n",
    "    Preprocessing of input numpy array:\n",
    "    1. resize image into predefined size\n",
    "    2. move color channel axis to a first place\n",
    "    \"\"\"\n",
    "    def __init__(self, *args):\n",
    "        super(InputWrapper, self).__init__(*args)\n",
    "        old_space = self.observation_space\n",
    "        assert isinstance(old_space, spaces.Box)\n",
    "        self.observation_space = spaces.Box(\n",
    "            self.observation(old_space.low), self.observation(old_space.high),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "    def observation(self, observation: gym.core.ObsType) -> gym.core.ObsType:\n",
    "        # resize image\n",
    "        new_obs = cv2.resize(\n",
    "            observation, (IMAGE_SIZE, IMAGE_SIZE))\n",
    "        # transform (w, h, c) -> (c, w, h)\n",
    "        new_obs = np.moveaxis(new_obs, 2, 0)\n",
    "        return new_obs.astype(np.float32)\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super(Discriminator, self).__init__()\n",
    "        # this pipe converges image into the single number\n",
    "        self.conv_pipe = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input_shape[0], out_channels=DISCR_FILTERS,\n",
    "                      kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=DISCR_FILTERS, out_channels=DISCR_FILTERS*2,\n",
    "                      kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(DISCR_FILTERS*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=DISCR_FILTERS * 2, out_channels=DISCR_FILTERS * 4,\n",
    "                      kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(DISCR_FILTERS * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=DISCR_FILTERS * 4, out_channels=DISCR_FILTERS * 8,\n",
    "                      kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(DISCR_FILTERS * 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=DISCR_FILTERS * 8, out_channels=1,\n",
    "                      kernel_size=4, stride=1, padding=0),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv_pipe(x)\n",
    "        return conv_out.view(-1, 1).squeeze(dim=1)\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, output_shape):\n",
    "        super(Generator, self).__init__()\n",
    "        # pipe deconvolves input vector into (3, 64, 64) image\n",
    "        self.pipe = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=LATENT_VECTOR_SIZE, out_channels=GENER_FILTERS * 8,\n",
    "                               kernel_size=4, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(GENER_FILTERS * 8),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(in_channels=GENER_FILTERS * 8, out_channels=GENER_FILTERS * 4,\n",
    "                               kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(GENER_FILTERS * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(in_channels=GENER_FILTERS * 4, out_channels=GENER_FILTERS * 2,\n",
    "                               kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(GENER_FILTERS * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(in_channels=GENER_FILTERS * 2, out_channels=GENER_FILTERS,\n",
    "                               kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(GENER_FILTERS),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(in_channels=GENER_FILTERS, out_channels=output_shape[0],\n",
    "                               kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pipe(x)\n",
    "\n",
    "\n",
    "def iterate_batches(envs: tt.List[gym.Env],\n",
    "                    batch_size: int = BATCH_SIZE) -> tt.Generator[torch.Tensor, None, None]:\n",
    "    batch = [e.reset()[0] for e in envs]\n",
    "    env_gen = iter(lambda: random.choice(envs), None)\n",
    "\n",
    "    while True:\n",
    "        e = next(env_gen)\n",
    "        action = e.action_space.sample()\n",
    "        obs, reward, is_done, is_trunc, _ = e.step(action)\n",
    "        if np.mean(obs) > 0.01:\n",
    "            batch.append(obs)\n",
    "        if len(batch) == batch_size:\n",
    "            batch_np = np.array(batch, dtype=np.float32)\n",
    "            # Normalising input to [-1..1] and convert to tensor\n",
    "            yield torch.tensor(batch_np * 2.0 / 255.0 - 1.0)\n",
    "            batch.clear()\n",
    "        if is_done or is_trunc:\n",
    "            e.reset()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--dev\", default=\"cpu\", help=\"Device name, default=cpu\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    device = torch.device(args.dev)\n",
    "    envs = [\n",
    "        InputWrapper(gym.make(name))\n",
    "        for name in ('Breakout-v4', 'AirRaid-v4', 'Pong-v4')\n",
    "    ]\n",
    "    shape = envs[0].observation_space.shape\n",
    "\n",
    "    net_discr = Discriminator(input_shape=shape).to(device)\n",
    "    net_gener = Generator(output_shape=shape).to(device)\n",
    "\n",
    "    objective = nn.BCELoss()\n",
    "    gen_optimizer = optim.Adam(params=net_gener.parameters(), lr=LEARNING_RATE,\n",
    "                               betas=(0.5, 0.999))\n",
    "    dis_optimizer = optim.Adam(params=net_discr.parameters(), lr=LEARNING_RATE,\n",
    "                               betas=(0.5, 0.999))\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    gen_losses = []\n",
    "    dis_losses = []\n",
    "    iter_no = 0\n",
    "\n",
    "    true_labels_v = torch.ones(BATCH_SIZE, device=device)\n",
    "    fake_labels_v = torch.zeros(BATCH_SIZE, device=device)\n",
    "    ts_start = time.time()\n",
    "\n",
    "    for batch_v in iterate_batches(envs):\n",
    "        # fake samples, input is 4D: batch, filters, x, y\n",
    "        gen_input_v = torch.FloatTensor(BATCH_SIZE, LATENT_VECTOR_SIZE, 1, 1)\n",
    "        gen_input_v.normal_(0, 1)\n",
    "        gen_input_v = gen_input_v.to(device)\n",
    "        batch_v = batch_v.to(device)\n",
    "        gen_output_v = net_gener(gen_input_v)\n",
    "\n",
    "        # train discriminator\n",
    "        dis_optimizer.zero_grad()\n",
    "        dis_output_true_v = net_discr(batch_v)\n",
    "        dis_output_fake_v = net_discr(gen_output_v.detach())\n",
    "        dis_loss = objective(dis_output_true_v, true_labels_v) + \\\n",
    "                   objective(dis_output_fake_v, fake_labels_v)\n",
    "        dis_loss.backward()\n",
    "        dis_optimizer.step()\n",
    "        dis_losses.append(dis_loss.item())\n",
    "\n",
    "        # train generator\n",
    "        gen_optimizer.zero_grad()\n",
    "        dis_output_v = net_discr(gen_output_v)\n",
    "        gen_loss_v = objective(dis_output_v, true_labels_v)\n",
    "        gen_loss_v.backward()\n",
    "        gen_optimizer.step()\n",
    "        gen_losses.append(gen_loss_v.item())\n",
    "\n",
    "        iter_no += 1\n",
    "        if iter_no % REPORT_EVERY_ITER == 0:\n",
    "            dt = time.time() - ts_start\n",
    "            print(f\"Iter {iter_no} in {dt:.2f}s: gen_loss={np.mean(gen_losses):.3e}, dis_loss={np.mean(dis_losses):.3e}\")\n",
    "            ts_start = time.time()\n",
    "            writer.add_scalar(\"gen_loss\", np.mean(gen_losses), iter_no)\n",
    "            writer.add_scalar(\"dis_loss\", np.mean(dis_losses), iter_no)\n",
    "            gen_losses = []\n",
    "            dis_losses = []\n",
    "        if iter_no % SAVE_IMAGE_EVERY_ITER == 0:\n",
    "            img = vutils.make_grid(gen_output_v.data[:64], normalize=True)\n",
    "            writer.add_image(\"fake\", img, iter_no)\n",
    "            img = vutils.make_grid(batch_v.data[:64], normalize=True)\n",
    "            writer.add_image(\"real\", img, iter_no)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d08605",
   "metadata": {},
   "source": [
    "1. **Line 40-61**: the `InputWrapper` is a wrapper around a Gym game, it includes several transformations:\n",
    "    1. resize the input image from $210\\times 160$ (standard Atari resolution) to $64\\times 64$\n",
    "    2. move color plane of the image from the last position to the first, to meet Pytorch convention of convolution layers (input tensor with shape `[channels, height, width]`)\n",
    "    3. cast the image from `bytes` to `float`\n",
    "\n",
    "2. **Line 64-121**: define $2$ `nn.Module` classes:\n",
    "    1. **Line 60-87**: `Disciminator` takes our scaled color image as input and, by applying $5$ layers of convolutions, converts it into a single number passed through a Sigmoid nonlinearity\n",
    "        - Sigmoid output interpretation: the probability that `Discriminator` thinks our input image is from the real dataset\n",
    "    2. **Line 90-117**: `Generator` takes as input a vector of random numbers (latent vector) and, by using the `transposed convolution` (aka `deconvolution`), converts this vector into a color image of the originial resolution\n",
    "    3. as input, we'll use screenshot from several Atari games played simultaneously by a random agent ![Sample screenshots from three Atari games](../images/figure_3-6.png)\n",
    "\n",
    "3. **Line 124-141**: images are combined in batches that are generated by `iterate_batches()` function:\n",
    "    - `iterate_batches()` infinitely samples the environment from the provided list, issues random actions, and remembers observations in the batch list\n",
    "    - when the batch becomes of the required size, we normalize the image, convert it to a tensor, and yield from the generator\n",
    "    - the check for non-zero mean of the observation is required due to a bug in one of the games to prevent flickering of an image\n",
    "\n",
    "4. **Line 144-213**: our `main` function, which prepares models and runs the training loops:\n",
    "    1. **Line 144-154**: process the command-line arguments (which could be only one optional argument, `-dev`, which specifies the device to use for computations) and create our environment pool with a wrapper applied\n",
    "        - this environment array will be passed to the `iterate_batches()` function later to generate training data\n",
    "    2. **Line 156-164**: create our classes - a summary writer, both networks, a loss function, $2$ optimizers. Why $2$?\n",
    "        - to train the discriminator, we need to show it both real and fake data samples with appropriate labels ($1$ for real and $0$ for fake)\n",
    "            - during this pass, we update only the discriminator's params\n",
    "        - after that, we pass both real and fake samples through the dicriminator again, but this time, the labels are $1$s for all samples and we update only the generator's weights\n",
    "            - the second pass teaches the generator how to fool the discriminator and confuse real samples with the generated ones\n",
    "    3. **Line 166-172**: define arrays to accumulate losses, iterator counters, and variables with the true and fake labels, also store the current timestamp to report the time elapsed after $100$ interations of training\n",
    "    4. **Line 174-180**: at the begining of the training loop, generate a random vector and pass it to the `Generator` network\n",
    "    5. **Line 182-190**: then train the discriminator by applying it twice:\n",
    "        1. once to the true data samples in our batch\n",
    "        2. once to the generated ones\n",
    "        - we need to call `detach()` function on the generator's output to prevent gradients of this training pass from flowing into the generator\n",
    "            - `detach()` is a tensor method, which makes a copy of it without connection to the parent's operation, i.e. detaching the tensor from the parent's graph\n",
    "    6. **Line 192-198**: now, it's the generator's training time:\n",
    "        - we pass the generator's output to the discriminator, but now we don't stop the gradients\n",
    "        - instead, we apply the `objective()` funtion with `True` labels\n",
    "        - it'll push our generator in the direction where the samples that it generates make the discriminator confuse them with the real data\n",
    "    7. **Line 200-213**: report losses and feed the image samples to TensorBoard\n",
    "\n",
    "At the begining, the generated images are completely random noise\n",
    "    - but after 10k-20k iterations, the generator becomes more and more proficient at its job and the generated images become more and more similar to the real game screenshots\n",
    "    - after 40k-50k iterations ![Sample images produces by the generator network](../images/figure_3-7.png)\n",
    "\n",
    "\n",
    "Let's take a look of the result visualization after running the code:\n",
    "1. Generator Loss: ![Genarator Loss](../images/ch03_gen_loss.png)\n",
    "2. Discriminator Loss: ![Discriminator Loss](../images/ch03_disc_loss.png)\n",
    "3. Example of real image at 197k iterations: ![Real image examples](../images/ch03_197k_iter_real_img.png)\n",
    "4. Generated images after 32k iterations: ![32k generated images](../images/ch03_32k_iter_fake%20gen.png)\n",
    "5. Generated images after 57k iterations: ![57k generated images](../images/ch03_32k_iter_fake%20gen.png)\n",
    "6. Generated images after 197k iterations: ![197k generated images](../images/ch03_32k_iter_fake%20gen.png)\n",
    "\n",
    "\n",
    "We'll simplify our code by using the add-on Pytorch library, Ignite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ba5e2b",
   "metadata": {},
   "source": [
    "## 9. Pytorch Ignite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1943b1b",
   "metadata": {},
   "source": [
    "1. Pytorch's flexibility comes with  price: too much code to be written to solve our problems. Sometimes, when we work on routine tasks, we don't need this flexibility\n",
    "    - a non-exhaustive list of topics that are an essential part of any DL training procedure, but require some code to be written:\n",
    "        1. data preparation and transformation, and the generation of batches\n",
    "        2. calculation of training metrics (e.g. loss values, accuracy, F1-scores, etc.)\n",
    "        3. periodical testing of the model being trained on the test and validation datasets\n",
    "        4. model checkpointing after some number of iterations or when a new best metrics is achieved\n",
    "        5. sending metrics into a monitoring tool like TensorBoard\n",
    "        6. hypreparameters change over time, like a learning rate decrease/increase schedule\n",
    "        7. writing training progress messages on the console\n",
    "    - as these tasks occur in any DL project, it quickly becomes cumbersome to write the same code over and over again\n",
    "    - the normal approach to solving the issue is to write the functionality once, wrap it into a library (and open sourced), and reuse it later\n",
    "    - several libraries for Pytorch that simplify the solving of common tasks:\n",
    "        1. `ptlearn`\n",
    "        2. `fastai`\n",
    "        3. `ignite`\n",
    "        4. for RL, [`PTAN`](https://github.com/Shmuma/ptan) written by the author\n",
    "    - [Pytorch ecosystem projects](https://landscape.pytorch.org/) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8b5aa6",
   "metadata": {},
   "source": [
    "### 9.1 Ignite concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78fde35",
   "metadata": {},
   "source": [
    "1. At a high level, [`Ignite`](https://pytorch-ignite.ai/) simplifies the writing of the training loop in Pytorch DL\n",
    "    - `Engine` clas: the central piece, loops over the data source, applying the processing function to the data batch\n",
    "    - `Events`: functions called at specific conditions of the training loop, could be at:\n",
    "        1. begining/end of the whole training process\n",
    "        2. begining/end of a training epoch (iteration over the data)\n",
    "        3. begining/end of a single batch processing\n",
    "    - additionally, custom events allow us to specify our functions to be called every $N$ events\n",
    "\n",
    "2. Example:\n",
    "```python\n",
    "from ignite.engine import Engine, Events\n",
    "\n",
    "def training(engine, batch):\n",
    "    optimizer.zero_grad()\n",
    "    x, y = prepare_batch()\n",
    "    y_out = model(x)\n",
    "    loss = loss_fn(y_out, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "engine = Engine(training)\n",
    "engine.run(data)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510a16b0",
   "metadata": {},
   "source": [
    "### 9.2 GAN training on Atari using Ignite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b780f2",
   "metadata": {},
   "source": [
    "Refer to [atari gan ignite script](04_atari_gan_ignite.py):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a56aba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import random\n",
    "import argparse\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from ignite.engine import Engine, Events\n",
    "from ignite.handlers import Timer\n",
    "from ignite.metrics import RunningAverage\n",
    "from ignite.contrib.handlers import tensorboard_logger as tb_logger\n",
    "\n",
    "import torchvision.utils as vutils\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Register Atari environments\n",
    "import ale_py\n",
    "gym.register_envs(ale_py)\n",
    "\n",
    "# Configure gymnasium logger - set minimum level directly\n",
    "gym.logger.min_level = gym.logger.WARN  # Use gym.logger.ERROR for less verbose output\n",
    "\n",
    "LATENT_VECTOR_SIZE = 100\n",
    "DISCR_FILTERS = 64\n",
    "GENER_FILTERS = 64\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# dimension input image will be rescaled\n",
    "IMAGE_SIZE = 64\n",
    "\n",
    "LEARNING_RATE = 0.0001\n",
    "REPORT_EVERY_ITER = 100\n",
    "SAVE_IMAGE_EVERY_ITER = 1000\n",
    "\n",
    "\n",
    "class InputWrapper(gym.ObservationWrapper):\n",
    "    \"\"\"\n",
    "    Preprocessing of input numpy array:\n",
    "    1. resize image into predefined size\n",
    "    2. move color channel axis to a first place\n",
    "    \"\"\"\n",
    "    def __init__(self, *args):\n",
    "        super(InputWrapper, self).__init__(*args)\n",
    "        old_space = self.observation_space\n",
    "        assert isinstance(old_space, spaces.Box)\n",
    "        self.observation_space = gym.spaces.Box(self.observation(old_space.low), self.observation(old_space.high),\n",
    "                                                dtype=np.float32)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        # resize image\n",
    "        new_obs = cv2.resize(observation, (IMAGE_SIZE, IMAGE_SIZE))\n",
    "        # transform (w, h, c) -> (c, w, h)\n",
    "        new_obs = np.moveaxis(new_obs, 2, 0)\n",
    "        return new_obs.astype(np.float32)\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super(Discriminator, self).__init__()\n",
    "        # this pipe converges image into the single number\n",
    "        self.conv_pipe = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input_shape[0], out_channels=DISCR_FILTERS,\n",
    "                      kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=DISCR_FILTERS, out_channels=DISCR_FILTERS*2,\n",
    "                      kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(DISCR_FILTERS*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=DISCR_FILTERS * 2, out_channels=DISCR_FILTERS * 4,\n",
    "                      kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(DISCR_FILTERS * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=DISCR_FILTERS * 4, out_channels=DISCR_FILTERS * 8,\n",
    "                      kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(DISCR_FILTERS * 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=DISCR_FILTERS * 8, out_channels=1,\n",
    "                      kernel_size=4, stride=1, padding=0),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv_pipe(x)\n",
    "        return conv_out.view(-1, 1).squeeze(dim=1)\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, output_shape):\n",
    "        super(Generator, self).__init__()\n",
    "        # pipe deconvolves input vector into (3, 64, 64) image\n",
    "        self.pipe = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=LATENT_VECTOR_SIZE, out_channels=GENER_FILTERS * 8,\n",
    "                               kernel_size=4, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(GENER_FILTERS * 8),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(in_channels=GENER_FILTERS * 8, out_channels=GENER_FILTERS * 4,\n",
    "                               kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(GENER_FILTERS * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(in_channels=GENER_FILTERS * 4, out_channels=GENER_FILTERS * 2,\n",
    "                               kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(GENER_FILTERS * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(in_channels=GENER_FILTERS * 2, out_channels=GENER_FILTERS,\n",
    "                               kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(GENER_FILTERS),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(in_channels=GENER_FILTERS, out_channels=output_shape[0],\n",
    "                               kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pipe(x)\n",
    "\n",
    "\n",
    "def iterate_batches(envs, batch_size=BATCH_SIZE):\n",
    "    batch = [e.reset()[0] for e in envs]\n",
    "    env_gen = iter(lambda: random.choice(envs), None)\n",
    "\n",
    "    while True:\n",
    "        e = next(env_gen)\n",
    "        obs, reward, is_done, is_trunc, _ = e.step(e.action_space.sample())\n",
    "        if np.mean(obs) > 0.01:\n",
    "            batch.append(obs)\n",
    "        if len(batch) == batch_size:\n",
    "            # Normalising input between -1 to 1\n",
    "            batch_np = np.array(batch, dtype=np.float32)\n",
    "            yield torch.tensor(batch_np * 2.0 / 255.0 - 1.0)\n",
    "            batch.clear()\n",
    "        if is_done or is_trunc:\n",
    "            e.reset()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--dev\", default=\"cpu\",\n",
    "                        help=\"Device name, default=cpu\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    device = torch.device(args.dev)\n",
    "    envs = [InputWrapper(gym.make(name)) for name in ('Breakout-v4', 'AirRaid-v4', 'Pong-v4')]\n",
    "    input_shape = envs[0].observation_space.shape\n",
    "\n",
    "    net_discr = Discriminator(input_shape=input_shape).to(device)\n",
    "    net_gener = Generator(output_shape=input_shape).to(device)\n",
    "\n",
    "    objective = nn.BCELoss()\n",
    "    gen_optimizer = optim.Adam(params=net_gener.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
    "    dis_optimizer = optim.Adam(params=net_discr.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
    "\n",
    "    true_labels_v = torch.ones(BATCH_SIZE, device=device)\n",
    "    fake_labels_v = torch.zeros(BATCH_SIZE, device=device)\n",
    "\n",
    "    def process_batch(trainer, batch):\n",
    "        gen_input_v = torch.FloatTensor(BATCH_SIZE, LATENT_VECTOR_SIZE, 1, 1)\n",
    "        gen_input_v.normal_(0, 1)\n",
    "        gen_input_v = gen_input_v.to(device)\n",
    "        batch_v = batch.to(device)\n",
    "        gen_output_v = net_gener(gen_input_v)\n",
    "\n",
    "        # train discriminator\n",
    "        dis_optimizer.zero_grad()\n",
    "        dis_output_true_v = net_discr(batch_v)\n",
    "        dis_output_fake_v = net_discr(gen_output_v.detach())\n",
    "        dis_loss = objective(dis_output_true_v, true_labels_v) + \\\n",
    "                   objective(dis_output_fake_v, fake_labels_v)\n",
    "        dis_loss.backward()\n",
    "        dis_optimizer.step()\n",
    "\n",
    "        # train generator\n",
    "        gen_optimizer.zero_grad()\n",
    "        dis_output_v = net_discr(gen_output_v)\n",
    "        gen_loss = objective(dis_output_v, true_labels_v)\n",
    "        gen_loss.backward()\n",
    "        gen_optimizer.step()\n",
    "\n",
    "        if trainer.state.iteration % SAVE_IMAGE_EVERY_ITER == 0:\n",
    "            fake_img = vutils.make_grid(gen_output_v.data[:64], normalize=True)\n",
    "            trainer.tb.writer.add_image(\"fake\", fake_img, trainer.state.iteration)\n",
    "            real_img = vutils.make_grid(batch_v.data[:64], normalize=True)\n",
    "            trainer.tb.writer.add_image(\"real\", real_img, trainer.state.iteration)\n",
    "            trainer.tb.writer.flush()\n",
    "        return dis_loss.item(), gen_loss.item()\n",
    "\n",
    "    engine = Engine(process_batch)\n",
    "    tb = tb_logger.TensorboardLogger(log_dir=None)\n",
    "    engine.tb = tb\n",
    "    RunningAverage(output_transform=lambda out: out[1]).\\\n",
    "        attach(engine, \"avg_loss_gen\")\n",
    "    RunningAverage(output_transform=lambda out: out[0]).\\\n",
    "        attach(engine, \"avg_loss_dis\")\n",
    "\n",
    "    handler = tb_logger.OutputHandler(tag=\"train\", metric_names=['avg_loss_gen', 'avg_loss_dis'])\n",
    "    tb.attach(engine, log_handler=handler, event_name=Events.ITERATION_COMPLETED)\n",
    "\n",
    "    timer = Timer()\n",
    "    timer.attach(engine)\n",
    "\n",
    "    @engine.on(Events.ITERATION_COMPLETED)\n",
    "    def log_losses(trainer):\n",
    "        if trainer.state.iteration % REPORT_EVERY_ITER == 0:\n",
    "            print(f\"Iter {trainer.state.iteration} in {timer.value():.2f}s: \"\n",
    "                  f\"gen_loss={trainer.state.metrics['avg_loss_gen']:.3e}, \"\n",
    "                  f\"dis_loss={trainer.state.metrics['avg_loss_dis']:.3e}\")\n",
    "            timer.reset()\n",
    "\n",
    "    engine.run(data=iterate_batches(envs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc1481f",
   "metadata": {},
   "source": [
    "1. **Line 9=12**: import `Ignite` classes:\n",
    "    1. `ignite.metrics` contains classes related to working with the performance metrics of the training process\n",
    "        - in this case, we use `RunningAverage` to smooth time series values\n",
    "        - previously we used `np.mean()` on an array of losses\n",
    "    2. `tensorboard_logger` from the `ignite.contrib` package (the functionality contributed by others)\n",
    "    3. `Timer` handler provides a simple way to calculate time elapsed between certain events\n",
    "\n",
    "2. **Line 159-188**: `process_batch()` function takes the data batch and does an update of both the discriminator and generator models on this batch\n",
    "    - can return any data to be tracked during the training process (in this case, $2$ loss values for both models)\n",
    "    - also save images to be displayed in TensorBoard\n",
    "\n",
    "3. **Line 190-202**: create our engine, passing our `process_batch()` function and attaching $2$ `RunningAverage` transformations for our $2$ loss values\n",
    "    - being attached, every `RunningAverage` produces a so-called `metric` - a derived value kept around during the training process\n",
    "    - names of our smoothed metrics:\n",
    "        1. `avg_loss_gen` for smoothed loss from the generator\n",
    "        2. `avg_loss_dis` for smoothed loss from the discriminator\n",
    "    - these $2$ values will be written in TensorBoard after every iteration\n",
    "    - also attach the timer, which being created without any constructor arguments, acts as a simple manually-controlled timer (we call its `reset()` method manually), but can work in a more flexible way with different configuration options\n",
    "\n",
    "4. **Line 204-212**: attach another event handler, which will be our function, and is called by `Engine` on every iteration completion:\n",
    "    - it'll write a log line with an iteration index, time taken and values of smoothed metrics\n",
    "    - **Line 212** starts our engine, passing the already defined function as the data source\n",
    "        - the `iterate_batches()` function is a generator, returning the normal iterator over batches, so it'll be perfectly fine to pass its output as a data argument"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deep-Reinforcement-Learning-Hands-On-Third-Edition",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
