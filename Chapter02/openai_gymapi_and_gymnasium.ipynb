{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c9d1ea9",
   "metadata": {},
   "source": [
    "\n",
    "# OpenAI Gym API and Gymnasium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90ab077",
   "metadata": {},
   "source": [
    "We'll learn:\n",
    "- the basics of `Gymnasium` (a fork of `OpenAI Gym` implementing the same API), a library used to provide a uniform API for an RL agent and lots of RL environments (previously, this API was implemented by `OpenAI Gym` library, but no longer maintained)\n",
    "- to write our first randomly behaving agent and become familiar with the basic concepts of RL that we covered so far"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8595b3a5",
   "metadata": {},
   "source": [
    "## 1. The anatomy of the agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058644fa",
   "metadata": {},
   "source": [
    "1. **The agent**: in practice, is some piece of code the implements some policy. Basically, this policy decides what action is needed at every time step, given our observations\n",
    "2. **The environment**: everything that is external to the agent and has the responsibility of providing observations and giving rewards. The environment changes its state based on the agent's actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a515f8a6",
   "metadata": {},
   "source": [
    "Let's define an environment that will give the agent random rewards for a limited number of steps, regardless of the agent's actions ([agent anatomy script](01_agent_anatomy.py)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b4be23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class Environment:\n",
    "    def __init__(self):\n",
    "        self.steps_left = 10\n",
    "\n",
    "    def get_observation(self) -> List[float]:\n",
    "        return [0.0, 0.0, 0.0]\n",
    "\n",
    "    def get_actions(self) -> List[int]:\n",
    "        return [0, 1]\n",
    "\n",
    "    def is_done(self) -> bool:\n",
    "        return self.steps_left == 0\n",
    "\n",
    "    def action(self, action: int) -> float:\n",
    "        if self.is_done():\n",
    "            raise Exception(\"Game is over\")\n",
    "        self.steps_left -= 1\n",
    "        return random.random()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b404b02",
   "metadata": {},
   "source": [
    "- **Line 5-7**: the environment initializes its internal state. In this case, the state is just a counter that limits the number of time steps that the agent is allowed to take to interact with the environment\n",
    "\n",
    "- **Line 9-10**: `get_observation()` method returns the current environment's observation to the agent. It is usually implemented as some function of the internal state of the environment\n",
    "    - in this case, the observation vector is always zero, as the environment basically has no internal state\n",
    "\n",
    "- **Line 12-13**: `get_actions()` method allows the agent to query the set of actions it can execute\n",
    "    - normally, the set of actions does not change over time, but some actions can become impossible in different states\n",
    "    - in this case, there are only two possible actions that the agent can carry out\n",
    "\n",
    "- **Line 15-16**: `is_done()` method signals the end of the episode to the agent\n",
    "\n",
    "- **Line 18-22**: `action()` method handles an agent's action and returns the reward for this action\n",
    "    - in this case, the reward is random and its action is discarded\n",
    "    - additionally, we update the count of steps and don't continue the episodes that are over\n",
    "\n",
    "\n",
    "Let's take a look at the agent:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30f72415",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.total_reward = 0.0\n",
    "\n",
    "    def step(self, env: Environment):\n",
    "        current_obs = env.get_observation()\n",
    "        actions = env.get_actions()\n",
    "        reward = env.action(random.choice(actions))\n",
    "        self.total_reward += reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825b2293",
   "metadata": {},
   "source": [
    "**Line 2-3**: the constructor initializes the counter that will keep the total reward accumulated by the agent during the episode\n",
    "\n",
    "**Line 5-9**: `step()` function accepts the environment as an argument. It allows the agent to perform the following actions:\n",
    "    - **Line 6**: observe the environment\n",
    "    - **Line 7**: make a decision about the action to take based on the observations\n",
    "    - **Line 8**: submit the action to the environment\n",
    "    - **Line 8-9**: get the reward for the current step\n",
    "\n",
    "In this case, the agent is dull and ignores the observations obtained during the decision-making process about which action to take. Instead, eveyr action is selected randomly\n",
    "\n",
    "Let's create both classes and runs one episode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd4bdd15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward got: 3.7976\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = Environment()\n",
    "    agent = Agent()\n",
    "\n",
    "    while not env.is_done():\n",
    "        agent.step(env)\n",
    "\n",
    "    print(\"Total reward got: %.4f\" % agent.total_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171a0656",
   "metadata": {},
   "source": [
    "## 2. Hardware and software requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e5aed9",
   "metadata": {},
   "source": [
    "1. The external libraries we'll use: \n",
    "    - `Numpy`\n",
    "    - `OpenCV Python bindings`: computer vision library, provide many functions for image processing\n",
    "    - `Gymnasium`: a maintained fork of OpenAI Gym library and an RL framework that has various environments that can be communicated with in a unified way\n",
    "    - `Pytorch`: a flexible and expressive DL library\n",
    "    - [`Pytorch Ignite`](https://pytorch-ignite.ai/): a set of high-level tools on top of Pytorch used to redude boilerplate code\n",
    "    - [`PTAN`](https://github.com/Shmuma/ptan): an open source extension to the OpenAI Gym API to support modern DL methods and building blocks\n",
    "\n",
    "2. refer to [Deep Reinforcement Learning Hands-on (Page 62)](../Deep%20Reinforcement%20Learning%20Hands-on%203rd%20Edition%20-%20Maxim%20Lapan.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a784c5b",
   "metadata": {},
   "source": [
    "## 3. The OpenAI Gym API and Gymnasium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fa098a",
   "metadata": {},
   "source": [
    "**Main goal of Gym**: provide a rich collection of environments for RL experiments using a unified interface:\n",
    "1. Central class in this library is an environment `Env`. Instances of this class expose several methods and fields that provide the required information about its capabilities\n",
    "2. At a high level, every environment provides these pieces of information and functionality:\n",
    "    - a set of actions that is allowed to be executed in the environment. Gym supports both discrete and continuous actions, as well as their combination\n",
    "    - the shape and boundaries of the observations that the environment provides the agent with\n",
    "    - a `step()` method that executes an action, returns the current observation, the reward, and a flag indicating that the episode is over\n",
    "    - a `reset()` method that returns the environment to its initial state and obtains the first observation\n",
    "\n",
    "Let's talk about these components of the environment in detail:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1c2eb5",
   "metadata": {},
   "source": [
    "### 3.1 The action space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a615ffc",
   "metadata": {},
   "source": [
    "The actions that an agent can execute can be discrete, continuous, or a combination of the two:\n",
    "1. **Discrete actions**: are a fixed set of things that an agent can do. Main characteristic is these states are mutually exclusive, only one action from a finite set of actions is possible at a time\n",
    "2. a **continuous action**: has a value attached to it. A description of a continuous action includes the boundaries of the value that the action could have\n",
    "3. the environment could take multiple actions. To support such cases, Gym defines a special container class that allows the nesting of several action spaces into one unified action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e00524",
   "metadata": {},
   "source": [
    "### 3.2 The observation space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a6388e",
   "metadata": {},
   "source": [
    "1. Observations are pieces of information that an environment provides the agent with, on every timestamp, besides the reward\n",
    "- You can see the similarity between actions and observations, and that is how they have been represented in Gyn's classes. Let's look at a class diagram: ![The hierarchy of the Space class in Gym](../images/figure_2-1.png)\n",
    "- the basic abstract `Space` class includes $1$ property and $3$ methods:\n",
    "    - `shape`: contain the shape of the space, identical to Numpy array\n",
    "    - `sample()`: return a random sample from the space\n",
    "    - `contains(x)`: check whether the argument `x` belongs to the space's domain\n",
    "    - `seed()`: initialize a random number generator for the space and all subspaces (useful for getting reproducible environment behavior accross several runs)\n",
    "- all these methods are abstract and reimplemented in each of the `Space` subclasses:\n",
    "    - `Discrete` class: represent a mutually exclusive set of items, numbered from $0$ to $n-1$\n",
    "        - can redefine the starting index with the optional constructor argument `start` if needed\n",
    "        - $n$ is a count of the items our `Discrete` object describes\n",
    "        - example: `Discrete(n=4)` used for an action space of $4$ directions to move in `[left, right, up, down]`\n",
    "    - `Box` class: represent an $n$-dimensions tensor of rational numbers with intervals `[low, high]`:\n",
    "        - examples:\n",
    "            1. `Box(low=0.0, high=1.0, shape=(1,), dtype=np.float32)`\n",
    "            2. `Box(low=0, high=255, shape=(210, 160, 3), dtype=np.unit8)`\n",
    "    - `Tuple` class: combine several `Space` class instances together. This enables us to create action and observation spaces of any complexity that we want:\n",
    "        - example:\n",
    "        ```python\n",
    "                    Tuple(spaces=(Box(low=-1.0, high=1.0, shape=(3,), dtype=np.float32),\n",
    "                                  Discrete(n=3),\n",
    "                                  Discrete(n=2)))\n",
    "        ```\n",
    "    - other `Space` subclasses:\n",
    "        - `Sequence`: represent variable-length sequences\n",
    "        - `Text`: strings\n",
    "        - `Graph`: where space is a set of nodes with connections between them\n",
    "\n",
    "2. Every environment has $2$ memebers of type `Space`: `action_space` and `observation_space`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69285aa4",
   "metadata": {},
   "source": [
    "### 3.3 The environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2208b99b",
   "metadata": {},
   "source": [
    "1. The environment is represented in Gym by the `Env` class, which has the following members:\n",
    "    - `action_space`: the field of the `Space` class and provides a specification for allowed actions in the environment\n",
    "    - `observation_space`: this field has the same `Space` class, but specifies the observations provided by the environment\n",
    "    - `reset()`: resets the environment to its initial state, returning the initial observation vector and the dict with extra information from the environment\n",
    "    - `step()`: allows the agent to take the action and returns information about the outcome of the action:\n",
    "        - the next observation\n",
    "        - the local reward\n",
    "        - the end-of-episode flag\n",
    "        - the flag indicating a truncated episode\n",
    "        - a dictionary with extra information from the environment\n",
    "    - `render()` (won't use): obtains the observation in a human-friendly form\n",
    "\n",
    "2. Let's focus on the core `Env` methods: `reset()` and `step()`\n",
    "\n",
    "3. `reset()` has no arguments; it instructs an environment to reset into its initial state and obtain the initial observation\n",
    "    - note: have to call `reset()` after the creation of the environment\n",
    "    - the agent's communication with the environment may have an end. Such sessions are called episodes, and after the end of the episode, an agent needs to start over\n",
    "    - the value returned by this method is the first observation of the environment\n",
    "    - `reset()` also returns the dictionary with extra environment-specific information (empty in most standard environments)\n",
    "\n",
    "4. `step()` - central piece in the environment's functionality\n",
    "    - it does several things in one call:\n",
    "        1. tell the environment which action we'll execute in the next step\n",
    "        2. get new observation from the environment after this action\n",
    "        3. get reward the agent gained with this step\n",
    "        4. get the indication that the episode is over\n",
    "        5. get the flag which signals an episode truncation (when time limit is enabled, for example)\n",
    "        6. ge the dict with extra environment-specific information\n",
    "    - the first item in the preceding list (`action`) i passed as the only argument to the `step()` method, and the rest are returned as a tuple of $5$ elements (`observation`, `reward`, `done`, `truncated`, and `info`). They have these types and meanings:\n",
    "        1. `observation`: Numpy vector/matrix with observation data\n",
    "        2. `reward`: float value of the reward\n",
    "        3. `done`: Boolen - `True` when the episode is over. If this value is `True`, we have to call `reset()` in the environment, as no more actions are possible\n",
    "        4. `truncated`: Boolen - `True` when the episode is truncated.\n",
    "            - for most environment, this is a `TimeLimit`, but might have different meaning in some environments\n",
    "            - if this value is `True`, we have to call `reset()` in the environment\n",
    "        5. `info`: extra environment-specific information. Usual practice: ignore this value in general RL methods\n",
    "\n",
    "5. environment usage in an agent's code:\n",
    "    1. in a loop, call `step()` method with an action to perform until `done` or `truncated` flags become `True`\n",
    "    2. then, call `reset()` to start over\n",
    "    3. a missing piece: how to create `Env` object?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61a4354",
   "metadata": {},
   "source": [
    "### 3.4 Creating an environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032c46f4",
   "metadata": {},
   "source": [
    "1. Every environment has a unique name of the `EnvironmentName-vN` form, where `N` is the number of used to distinguish between different versions of the same environment\n",
    "    - use `gymnasium`'s `make(name)` function to create an environment\n",
    "\n",
    "2. Same environment can have different variations in the settings and observations spaces. For example, the **Atari** game **Breakout** has these environment names:\n",
    "    - `Breakout-v0`, `Breakout-v4`: original Breakout with a random initial position and direction of the ball\n",
    "    - `BreakoutDeterministic-v0`, `BreakoutDeterministic-v4`: Breakout with the same initial placement and speed vector of the ball\n",
    "    - `BreakoutNoFrameskip-v0`, `BreakoutNoFrameskip-v4`: Breakout with every frame displayed to the agent. Without this, every action is executed for several consecutive frames\n",
    "    - `Breakout-ram-v0`, `Breakout-ram-v4`: Breakout with the observation of the full Atari emulation memory (128 bytes) instead of screen pixels\n",
    "    - `Breakout-ramDeterministic-v0`, `Breakout-ramDeterministic-v4`: memory observation with the same initial state\n",
    "    - `Breakout-ramNoFrameskip-v0`, `Breakout-ramNoFrameskip-v4`: memory observation without frame skipping\n",
    "\n",
    "3. Even after removal of such duplicates, Gymnasium comes with an impressive list of $198$ unique environments, which can be divided into several groups:\n",
    "    - `classic control problems`: toy tasks used in optimal control theory and RL papers as benchmarks or demonstrations\n",
    "        - usually simple, with low-dimension observation and action spaces, but are useful as quick checks when implementing algorithms\n",
    "        - think about them as `MNIST for RL`\n",
    "    - `Atari 2600`: games from classic game platform from 1970s. There are 63 unique games\n",
    "    - `Algorithmic`: problems aim to perform small computation tasks, such as copying the observed sequence or adding numbers\n",
    "    - `Box2D`: environments that use the Box2D physics simulator to learn walking or car control\n",
    "    - `MuJoCo`: physics simulator used for several continuous control problems\n",
    "    - `Parameter tuning`: RL used to optimize NN parameters\n",
    "    - `Toy text`: simple grid world text environments\n",
    "\n",
    "4. Total number of RL environments supporting the `Gym API` is much larger:\n",
    "    - the `Farama Foundation` maintains several repos related to special RL topics like multi-agent RL, 3D navigation, robotics, and web automation\n",
    "    - [third-party repos](https://gymnasium.farama.org/environments/third_party_environments/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d017cf5",
   "metadata": {},
   "source": [
    "### 3.5 The CartPole session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0dc98ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "e = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23a0a9e",
   "metadata": {},
   "source": [
    "We call the `CartPole` environment from the `gymnasium` package\n",
    "- this environment is from the classic control group and its gist it to control the platform with a stick attached to its bottom part\n",
    "- the observation is $4$ floating-point numbers containing information about:\n",
    "    1. `x` coordinate of the stick's center of mass\n",
    "    2. its `speed`\n",
    "    3. its `angle` to the platform\n",
    "    4. its `angular speed`\n",
    "- the reward is $1$, given on every time step\n",
    "- the episode continues until the stick falls, so to get a more accumulated reward, we need to balance the platform in a way to avoid the stick falling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eeb5fd31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.02871413, -0.01405009, -0.0225007 ,  0.04278427], dtype=float32),\n",
       " {})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs, info  = e.reset()\n",
    "obs, info        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8dfc93",
   "metadata": {},
   "source": [
    "We reset the environment and obtain the first observation, then we examine the action space and observation space:\n",
    "- `action_space` field is of `Discrete` type, so our actions will be just $0$ and $1$:\n",
    "    - $0$: push the platform to the left\n",
    "    - $1$: push the platform to the right\n",
    "- `observation_space` is of `Box(4,)`, i.e. a vector of length $4$\n",
    "    - the first list is the low bound and the second is the high bound of parameters\n",
    "    - refer to [Gymnasium repo - cartpole.py](https://github.com/Farama-Foundation/Gymnasium/blob/main/gymnasium/envs/classic_control/cartpole.py#L40) for meanings of these values\n",
    "        1. `Cart position`: in $[-4.8, 4.8]$\n",
    "        2. `Cart velocity`: in $[-\\infty, \\infty]$\n",
    "        3. `Pole angle`: in $[-0.418, 0.418]$\n",
    "        4. `Pole angular velocity`: in $[-\\infty, \\infty]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "23e307d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "95559e5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([-4.8               -inf -0.41887903        -inf], [4.8               inf 0.41887903        inf], (4,), float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2ecc6e1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.02843313, -0.20884228, -0.02164502,  0.32828394], dtype=float32),\n",
       " 1.0,\n",
       " False,\n",
       " False,\n",
       " {})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.step(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb46d3c9",
   "metadata": {},
   "source": [
    "Let's send an action to the environment:\n",
    "- `e.step(0)` pushed platform to the left by executing the action `0` and got a tuple of $5$ elements:\n",
    "    1. a new observation: a new vector of length $4$\n",
    "    2. a reward of $1.0$\n",
    "    3. `done` flag with value `False` --> episode is not over yet and we are more or less okay with balancing the pole\n",
    "    4. `truncated` flag with value `False` --> episode was not truncated\n",
    "    5. extra information about the environment - an empty dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "65719125",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ea8a1a3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(1)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3e48d081",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.49597013, 1.0956569 , 0.23275931, 1.0554833 ], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0818c47a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.4710777 , -0.409212  ,  0.14004734,  0.3030727 ], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.observation_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1458e0",
   "metadata": {},
   "source": [
    "We used the `sample()` method of the `Space` class on the `action_space` and `observation_space`:\n",
    "- this method return a random sample from the underlying space\n",
    "- `Discrete` `action_space` will return a random number of $0$ or $1$ (could be used when we're not sure how to perform an action)\n",
    "- `observation_space` will return a ramdom vector of length $4$ (not very useful)\n",
    "- this feature is especially handy because we don't know any RL methods yet, but still want to play around with the Gym environment\n",
    "\n",
    "Now, let's implement our first randomly behaving agent for `CartPole`:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d6cc46",
   "metadata": {},
   "source": [
    "## 4. The random CartPole agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7deeb3",
   "metadata": {},
   "source": [
    "1. Refer to [cartpole random script](02_cartpole_random.py):\n",
    "    - **Line 4-8**: create the environment and initialize the counter of steps and the reward accumulator:\n",
    "        - **Line 8**: reset the environment to obtain the first observation (which we'll not use, as our agent is stochastic)\n",
    "    - **Line 10-18**: in the loop, after sampling a random action, we ask the environment to execute it and return to us the next observation `obs`, the `reward`, the `is_done`, and the `is_trunc` flags\n",
    "        - if the episode is over, we stop the loop and show how many steps we have taken and how much reward has been accumulated\n",
    "\n",
    "2. Most of the environments in Gym have a `reward boundary`, which is the average reward that the agent should gain during $100$ consecutive episodes to \"solve\" the environment\n",
    "    - for `CartPole`, this boundary is $195$, i.e. on average, the agent must hold the stick for $195$ time steps or longer\n",
    "    - using this perspective, our random agent's performance looks poor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7560eeb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode done in 44 steps, total reward 44.00\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "    total_reward = 0.0\n",
    "    total_steps = 0\n",
    "    obs, _ = env.reset()\n",
    "\n",
    "    while True:\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, is_done, is_trunc, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        total_steps += 1\n",
    "        if is_done:\n",
    "            break\n",
    "\n",
    "    print(\"Episode done in %d steps, total reward %.2f\" % (total_steps, total_reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44fd380",
   "metadata": {},
   "source": [
    "## 5. Extra Gym API functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b47339",
   "metadata": {},
   "source": [
    "The rest of the API we can live without, but it will make our life easier and the code cleaner. Let's briefly cover them:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6b74be",
   "metadata": {},
   "source": [
    "\n",
    "### 5.1 Wrappers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76812d7",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "1. Very frequently, we'll want to extend the environment's functionality in some generic way. E.g.\n",
    "    1. imagine an environment gives us some observations, but we want to accumulate them in some buffer and provide to the agent the last $N$ observations (common scenario for computer games, when one single frame is just not enough to get the full information about the game state)\n",
    "    2. to be able to crop or preprocess an image's pixels to make it more convenient for the agent to digest\n",
    "    3. to normalize the reward scores\n",
    "\n",
    "2. There are many such situation that have the same structure - we want to \"wrap\" the existing environment and add some extra logic for doing something. Gym's `Wrapper` class: ![The hierarchy of the Wrapper class in Gym](../images/figure_2-4.png)\n",
    "    - inherit the `Env` class\n",
    "    - constructor accepts the only argument - the instance of the `Env` class to be wrapped\n",
    "    - to add extra functionality, redefine the methods we want to extend, such as `step()` or `reset()`. The only requirement is to call the original method of the superclass\n",
    "    - to simplify accessing the environment being wrapped, `Wrapper` has $2$ properties:\n",
    "        1. `env`, of the immediate environment we're wrapping (which could be another wrapper as well), and\n",
    "        2. `unwrapped`, which is an `Env` without any wrappers\n",
    "\n",
    "3. To handle more specific requirements, such as a `Wrapper` class that want to process only observations from the environment, or only actions, there are subclasses of `Wrapper` that allow the filtering of only a specific portion of information. They are as follows:\n",
    "    1. `ObservationWrapper`: need to redefine the `observation(obs)` method of the parent\n",
    "        - `obs` arugument is an observation from the wrapped environment, and this method should return the observation that will be given to the agent\n",
    "    2. `RewardWrapper`: this expose the `reward(rew)` method, which can modify the reward value given to the agent\n",
    "        - example: scale it to the needed range, add a discount based on some previous actions, etc.\n",
    "    3. `ActionWrapper`: need to override `action(a)` method, which can tweak the action passed to the wrapped environment by the agent\n",
    "\n",
    "4. To make it slightly practical, let's imagine a situation where we want to intervene in the stream of actions sent by the agent and, with a probability of $10%$, replace the current action with a random one\n",
    "    - might look unwise, but this simple trick is one of the mose practical and powerful methods for solving the `exploration/exploitation problem`\n",
    "    - by issuing random actions, we make our agent explore the environment and from time to time drift awat from the beaten track of its policy\n",
    "    - this is an easy thing to do using the `ActionWrapper` class\n",
    "\n",
    "5. Refer to [random action wrapper script](03_random_action_wrapper.py):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3cf863f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import random\n",
    "\n",
    "\n",
    "class RandomActionWrapper(gym.ActionWrapper):\n",
    "    def __init__(self, env: gym.Env, epsilon: float = 0.1):\n",
    "        super(RandomActionWrapper, self).__init__(env)\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def action(self, action: gym.core.WrapperActType) -> gym.core.WrapperActType:\n",
    "        if random.random() < self.epsilon:\n",
    "            action = self.env.action_space.sample()\n",
    "            print(f\"Random action {action}\")\n",
    "            return action\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0364aa5",
   "metadata": {},
   "source": [
    "- **Line 5-8**: initialize wrapper by calling a parent's `__init__()` method and saving `epsilon` (the probability of a random action)\n",
    "- **Line 10-15**: override `action()` method from a parent's class to tweak the agent's actions\n",
    "    - note: using `action_space` and wrapper abstraction, we were able to write abstract code, which will work with any environment from Gym\n",
    "\n",
    "Let's apply our wrapper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "70b29f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random action 0\n",
      "Random action 0\n",
      "Random action 0\n",
      "Reward got: 8.00\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = RandomActionWrapper(gym.make(\"CartPole-v1\"))\n",
    "\n",
    "    obs = env.reset()\n",
    "    total_reward = 0.0\n",
    "\n",
    "    while True:\n",
    "        obs, reward, done, _, _ = env.step(0)\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    print(f\"Reward got: {total_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808676ea",
   "metadata": {},
   "source": [
    "- **Line 2**: create a normal `CartPole` environment and pass it to our `Wrapper` constructor\n",
    "    - from here on, we'll use our wrapper as a normal `Env` instance, instead of the original `CartPole`\n",
    "    - as the `Wrapper` class inherits `Env` class and exposes the same interface, we can nest our wrappers as deep as we want. This is a powerful, elegant, and generic solution\n",
    "- **Line 4-13**: almost the same code as in the random agent, except that every time, we issue the same action $0$, so our agent is dull and does the same thing\n",
    "\n",
    "Let's look at how we render environment during execution:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31e6dbe",
   "metadata": {},
   "source": [
    "### 5.2 Rendering the environment (works with Python scripts, not works with Jupyter notebooks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd57ff4",
   "metadata": {},
   "source": [
    "It is implemented with $2$ wrappers:\n",
    "1. `HumanRendering`: open a separate graphical window in which the image from the environment is being shown interactively\n",
    "    - to be able to render the environment (`CartPole` in this case), it has to be initialized with the `render_mode=\"rbg_array\"` argument\n",
    "    - this argument tells the environment to return pixels from its `render()` method, which is being called by the `HumanRendering` wrapper\n",
    "    - refer to [cartpole random monitor script](04_cartpole_random_monitor.py):\n",
    "2. `RecordVideo`: captures the pixels from the environment and produces a video file of our agent in action\n",
    "    - used in the same way as `HumanRendering`, but requires an extra argument specifying the directory to store video files (will create one if directory doesn't exist)\n",
    "    - especially useful in situations when we're running our agent on a remote machine without the GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3b45126d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\Desktop\\Deep-Reinforcement-Learning-Hands-On-Third-Edition\\.venv\\Lib\\site-packages\\pygame\\pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode done in 18 steps, total reward 18.00\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "    env = gym.wrappers.HumanRendering(env)\n",
    "    # env = gym.wrappers.RecordVideo(env, video_folder=\"video\")\n",
    "\n",
    "    total_reward = 0.0\n",
    "    total_steps = 0\n",
    "    obs = env.reset()\n",
    "\n",
    "    while True:\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, done, _, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        total_steps += 1\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    print(f\"Episode done in {total_steps} steps, total reward {total_reward:.2f}\")\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4a84a7",
   "metadata": {},
   "source": [
    "Run the code in a Python script (`python .\\Chapter02\\04_cartpole_random_monitor.py`), the window with environment rendering will appear:\n",
    "    - as our agent cannot balance the pole for too long (10-30 steps max), the window will disappear quite quickly, once the `env.close()` method is called"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e1844b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode done in 53 steps, total reward 53.00\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "    # env = gym.wrappers.HumanRendering(env)\n",
    "    env = gym.wrappers.RecordVideo(env, video_folder=\"video\")\n",
    "\n",
    "    total_reward = 0.0\n",
    "    total_steps = 0\n",
    "    obs = env.reset()\n",
    "\n",
    "    while True:\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, done, _, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        total_steps += 1\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    print(f\"Episode done in {total_steps} steps, total reward {total_reward:.2f}\")\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23713155",
   "metadata": {},
   "source": [
    "### 5.3 More wrappers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a3ceba",
   "metadata": {},
   "source": [
    "Refer to [Gymnasium Wrappers]( https://gymnasium.farama.org/api/wrappers/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deep-Reinforcement-Learning-Hands-On-Third-Edition",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
